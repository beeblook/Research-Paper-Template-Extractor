{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLPResearchPaperTemplates.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"UfyqD8y-5WIF","colab_type":"text"},"cell_type":"markdown","source":["\n","All necessorry installation is done before executing the code"]},{"metadata":{"id":"5sAvkxPHRqki","colab_type":"code","outputId":"67bff10a-f652-40a7-ba6b-ee0bea216ed8","executionInfo":{"status":"ok","timestamp":1544575610717,"user_tz":360,"elapsed":6122,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["!pip install msgpack==0.5.6\n","\n","\n","import os\n","import nltk\n","from nltk.corpus import wordnet as wn\n","import re\n","\n","\n","\n","\n","import spacy\n","import spacy.cli\n","from spacy.lemmatizer import Lemmatizer\n","from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n","from spacy import displacy\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: msgpack==0.5.6 in /usr/local/lib/python3.6/dist-packages (0.5.6)\n"],"name":"stdout"}]},{"metadata":{"id":"LlUqwZN9jhLO","colab_type":"code","outputId":"406ed06d-acb9-47c1-94f0-d7afef44986a","executionInfo":{"status":"ok","timestamp":1544575610995,"user_tz":360,"elapsed":6354,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":836}},"cell_type":"code","source":["#download database\n","nltk.download('popular')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"JluspFxk4OgN","colab_type":"code","outputId":"6153d02a-911f-4201-df2f-5938e508f60d","executionInfo":{"status":"ok","timestamp":1544575613336,"user_tz":360,"elapsed":8661,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"cell_type":"code","source":["spacy.cli.download(\"en\")\n","nlp = spacy.load(\"en\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n","\n","    You can now load the model via spacy.load('en')\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Lru849Pbr7p8","colab_type":"code","colab":{}},"cell_type":"code","source":["def getNounChunks(doc):\n","  nounChunkList = []\n","  for chunk in doc.noun_chunks:\n","    nounChunkList.append([chunk.text, chunk.root.text, chunk.root.dep_,\n","          chunk.root.head.text])\n","  #for -ends\n","  return nounChunkList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qz-f1RY3jSUy","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- getNERFeatures() ---------------------------------|\n","def getNERFeatures(doc):  \n","  nerDict = {}\n","  for x in doc.ents:\n","    nerDict[x.text]=x.label_\n","  #for -ends\n","  return nerDict\n","# ------------------------- getNERFeatures() -ends ----------------------------|"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3hP5Vgq5i71y","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- getTokens() ---------------------------------------|\n","def getTokens(sentence):\n","  '''3.1 Tokenize the FAQ and Answers into sentence and words'''\n","  tokens = nltk.word_tokenize(sentence)\n","  return (tokens)\n","# ------------------------- getTokens() -ends ---------------------------------|"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b3M6X4VjjBIn","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- getFeatures() -------------------------------------|\n","\n","#3.2 Lemmatize the words to extract the lemmas as feature\n","#3.3 Part-of-speech (POS) tag the words to extract POS tag features\n","\n","def getFeatures(doc, featureList):\n","  for token in doc:\n","    if token.dep_ != 'punct' and token.dep_!='':\n","      featureDict = {}\n","      featureDict[\"token\"] = token\n","      featureDict[\"lemmaVal\"] = token.lemma\n","      featureDict[\"lemma\"] = token.lemma_\n","      featureDict[\"pos\"] = token.pos_\n","      featureDict[\"tag\"] = token.tag_\n","      featureDict[\"shape\"] = token.shape_\n","      featureDict[\"isAlpha\"] = token.is_alpha\n","      featureDict[\"isStop\"] = token.is_stop\n","      featureDict[\"dependency\"]=token.dep_\n","      featureDict[\"headText\"] = token.head.text\n","      featureDict[\"headTag\"]=token.head.pos_\n","      featureDict[\"children\"] = [child for child in token.children]\n","      featureList.append(featureDict)\n","  #for token -ends\n","  \n","  return featureList\n","# ------------------------- getFeatures() -ends--------------------------------|\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qc8pBX0sjOAB","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- wordnetFeatures() ---------------------------------|\n","\n","def wordnetFeatures(tokens):\n","  print(tokens, \"\\n\")\n","  wordnetTokenDict = {}\n","  \n","  for token in tokens:\n","    tokenSynset = wn.synsets(token)\n","#     print(\"------------------------------------------------------------------\")\n","#     print(\"token: {}\".format(token))\n","#     print(\"tokenSynset: {}\".format(tokenSynset))\n","    \n","    synonyms=[]\n","    hypernymList=[]\n","    hyponymList=[]\n","    holonymList=[]\n","    meronymList=[]\n","    entailList=[]\n","  \n","    tempTokenDict={}\n","    for index, tempSynset in enumerate(tokenSynset):\n","      \n","      hypernyms = tempSynset.hypernyms()\n","      for l in hypernyms:\n","        hypernymList.append(l.lemma_names()[0])\n","        \n","      hyponyms = tempSynset.hyponyms()\n","      for l in hyponyms:\n","        hyponymList.append(l.lemma_names()[0])\n","        \n","      holonyms = tempSynset.part_holonyms()\n","      for l in holonyms:\n","        holonymList.append(l.lemma_names()[0])\n","        \n","      meronyms = tempSynset.part_meronyms()\n","      for l in meronyms:\n","        meronymList.append(l.lemma_names()[0])\n","        \n","      entail = tempSynset.entailments()\n","      for l in entail:\n","        entailList.append(l.lemma_names()[0])\n","        \n","      for l in tempSynset.lemmas():\n","        synonyms.append(l.name())\n","     \n","#       print(\"current Synset: {}\".format(tempSynset))\n","#       print(\"hypernyms: {}\".format(hypernyms))\n","#       print(\"hyponyms: {}\".format(hyponyms))\n","#       print(\"holonyms: {}\".format(holonyms))\n","#       print(\"meronyms: {}\".format(meronyms))\n","#       print(\"entail = {}\".format(entail))\n","#       print(\"\\n\")\n","      \n","    tempTokenDict[\"hypernym\"]=hypernymList  \n","    tempTokenDict[\"hyponym\"]=hyponymList\n","    tempTokenDict[\"holonym\"]=holonymList\n","    tempTokenDict[\"meronym\"]=meronymList\n","    tempTokenDict[\"entail\"]=entailList\n","    tempTokenDict[\"synonym\"]=synonyms\n","    \n","    wordnetTokenDict[token]=tempTokenDict    \n","#     print(\"Synonym: {}\".format(synonyms))\n","  \n","  return wordnetTokenDict\n","    \n","  #apply lesk...\n","# ------------------------- wordnetFeatures() ---------------------------------|"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QM62HmWEjZpv","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- acknowledgementTemplate() -------------------------|\n","def getAcknowledgementTemplate(doc, sentence):\n","  '''\n","  input: doc, featureList (List)\n","  output: ackTemplateDict (dictionary)\n","          Authors:\n","          paper:\n","          publication:\n","          publish year:\n","          reference page:\n","  '''\n","  NERDict = getNERFeatures(doc)\n","  publicationList = [\"IEEE\", \"ACM\",\"NIPS\", \"arXiv\", \"IBM\", \"dissertation\", \"Cambridge\", \"Univ.\" \"Press\", \"McGraw-Hill\", \"Cornell university\"]\n","  \n","  ackTemplateDict = {\"authors\":[], \"paper\":\"\", \"publication\": \"\", \"publish-year\":\"\", \"reference-Page\": \"\"}\n","  authorList = []\n","  for key in NERDict:\n","    if NERDict[key]==\"PERSON\":\n","      authorList.append(key)\n","    elif NERDict[key]==\"DATE\":\n","      ackTemplateDict[\"publish-year\"]=key\n","    elif NERDict[key]==\"CARDINAL\":\n","      ackTemplateDict[\"reference-Page\"]=key\n","    elif NERDict[key]==\"ORG\" or NERDict[key]==\"GPE\":\n","      ackTemplateDict[\"publication\"]+=\" \"+key  \n","  #for key -ends\n","  \n","  for token in doc:\n","    for publication in publicationList:\n","      if publication.upper() in str(token).upper():\n","        ackTemplateDict[\"publication\"]+=\" \"+str(token)  \n","      #if -ends\n","    #forr publication -ends\n","  #for token -ends\n","  \n","  paper=re.findall(r'“(.*?)”',sentence)\n","  if len(paper)!=0:\n","    ackTemplateDict[\"paper\"]=paper[0]\n","  \n","    \n","  \n","  ackTemplateDict[\"authors\"]=authorList\n","  return ackTemplateDict    \n","# ------------------------- acknowledgementTemplate() --------------------------|"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jFYVJsuBul89","colab_type":"code","colab":{}},"cell_type":"code","source":["def getDependencyTree(doc):\n","    nodes = []\n","    for token in doc:\n","        nodes.append([token.text, token.dep_, token.head.text, token.head.pos_,\n","          [child for child in token.children]])\n","    return nodes"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vKz1gMjInwwq","colab_type":"code","colab":{}},"cell_type":"code","source":["def getAllChildren(sentence, tokenIndex):\n","  doc = nlp(sentence)\n","  return [token for token in doc[tokenIndex].children]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cFfNYX4wunOj","colab_type":"code","colab":{}},"cell_type":"code","source":["def printDependencies(sentence):\n","    nodes = get_dependency_tree_nodes(sentence)\n","    print(\"\\n\\n------------- Deps ------\")\n","    for node in nodes:\n","        print(node)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tQ2wHvVQz70F","colab_type":"code","colab":{}},"cell_type":"code","source":["def getVerbWithSubject(sentence):\n","  doc = nlp(sentence)\n","  verbs = set()\n","  for possible_subject in doc:\n","    if possible_subject.dep_ == 'nsubj' and possible_subject.head.pos == 'VERB':\n","      verbs.add(possible_subject.head)\n","  return (verbs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f-DLO_eSz8i7","colab_type":"code","colab":{}},"cell_type":"code","source":["def getLeftChildren(sentence, tokenIndex):\n","  doc = nlp(sentence)\n","  return [token for token in doc[tokenIndex].lefts]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oDVmXrsn0Dst","colab_type":"code","colab":{}},"cell_type":"code","source":["def getRightChildren(sentence, tokenIndex):\n","  doc = nlp(sentence)\n","  return [token for token in doc[tokenIndex].rights]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xqYVTq7e0EVb","colab_type":"code","colab":{}},"cell_type":"code","source":["def getRoot(sentence):\n","  doc = nlp(sentence)\n","  return [token for token in (doc) if token.head == token][0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CkeeLvh50Hh7","colab_type":"code","colab":{}},"cell_type":"code","source":["def getAncestors(sentence, descendant):\n","  doc = nlp(sentence)\n","  return  [ancestor for ancestor in descendant.ancestors]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jCoSZ9aW0JfR","colab_type":"code","colab":{}},"cell_type":"code","source":["def getPhraseSpan(sentence, tokenIndex):\n","  doc = nlp(sentence)\n","  span = doc[doc[tokenIndex].left_edge.i : doc[tokenIndex].right_edge.i+1]\n","  span.merge()\n","  return (span)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6kX2EAe1Tb9q","colab_type":"code","colab":{}},"cell_type":"code","source":["def getVerbSubject(sentence, tokenIndex):\n","  doc = nlp(sentence)\n","  return [token for token in doc[tokenIndex].children if token.dep_==\"nsubj\" or token.dep_==\"nsubjpass\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WqWYWEvsULZC","colab_type":"code","colab":{}},"cell_type":"code","source":["def getAllSubject(sentence):\n","  doc = nlp(sentence)\n","  sub =[]\n","  for token in doc:\n","    if token.dep_ == 'nsubj' or token.dep_=='nsubjpass':\n","      sub.append(token)\n","  return (sub)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2egwW4QFjAGj","colab_type":"code","colab":{}},"cell_type":"code","source":["def removeWords(query, removeWordList):\n","  querywords = query.split()  \n","  resultwords  = [word for word in querywords if word.lower() not in removeWordList]\n","  result = ' '.join(resultwords)\n","  return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yOtTBORb1Xg5","colab_type":"code","colab":{}},"cell_type":"code","source":["def getAllVerbs(sentence):\n","  doc = nlp(sentence)\n","  verbs =[]\n","  for token in doc:\n","    if token.pos_ == 'VERB':\n","      verbs.append(token)\n","  return (verbs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LJ2XPPSq0LNW","colab_type":"code","colab":{}},"cell_type":"code","source":["def getTokenIndex(sentence, inToken):\n","  doc = nlp(sentence)\n","\n","  tokenIndex = -1\n","  for index, token in enumerate(doc):\n","    if str(token)==str(inToken):\n","      return index\n","    #if -ends\n","  #for -ends\n","  return tokenIndex\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WhxqmgYj0Qbc","colab_type":"code","colab":{}},"cell_type":"code","source":["def getTask(sentence, rootIndex):\n","  doc = nlp(sentence)\n","  #ckeft children for domain and task\n","  leftChildrenList = getLeftChildren(sentence, rootIndex)\n","  for childToken in leftChildrenList:\n","    #skip punct\n","    if childToken.dep_ != 'punct' and childToken.dep_!='':\n","      #search for nsubj or nsubjpass\n","      if childToken.dep_ == 'nsubj' or childToken.dep_ == 'nsubjpass':\n","        taskChildIndex = getTokenIndex(sentence, childToken)\n","        if taskChildIndex!=-1:\n","          taskSpan = getPhraseSpan(sentence, taskChildIndex)\n","          return taskSpan.text\n","      #if taskChildIndex -ends\n","  return \"\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"BUx1d7Sm0UAY","colab_type":"code","colab":{}},"cell_type":"code","source":["def getDomain(sentence, rootIndex):\n","  doc = nlp(sentence)\n","  #ckeft children for domain and task\n","  leftChildrenList = getLeftChildren(sentence, rootIndex)\n","\n","  for childToken in leftChildrenList:\n","    #skip punct\n","    if childToken.dep_ != 'punct' and childToken.dep_!='':\n","      #look for prep\n","      if childToken.dep_=='prep':\n","        domainChildIndex = getTokenIndex(sentence, childToken)\n","        if domainChildIndex!=-1:\n","          domainSpan = getPhraseSpan(sentence, domainChildIndex)\n","          return domainSpan.text\n","  return \"\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"dkZtAUpm0Xo-","colab_type":"code","colab":{}},"cell_type":"code","source":["# ------------------------- getResultTemplate() -------------------------------|\n","def getResultTemplate(doc, sentence, featureList):\n","  '''\n","  input: doc, sentence, featureList (List)\n","  output: resultDict (dictionary)\n","          domain:\n","          task:\n","          attribute:\n","          result:\n","  '''\n","  #initializing dictionary\n","  resultDict = {\"domain\":\"\", \"task\": \"\", \"result\":\"\"}\n","  \n","  #initializing indecator list\n","  resultVerbList = [\"yield\", \"give\", \"render\",\"return\", \"generate\", \"predict\", \"foretell\", \"produce\", \"lead\", \"forcast\", \"report\", \"achieve\", \"accomplish\", \"reach\", \"contain\"]\n","  resultNounList = [\"result\", \"outcome\", \"solution\",\"effect\", \"decision\", \"answer\", \"final result\", \"score\", \"AP\",\"precision\", \"recall\", \"accuracy\", \"fruit\", \"fruition\", \"r\"]\n","  \n","  #check for indicator words\n","  positiveFlag = False\n","  for index, feature in enumerate(featureList):\n","    if (feature[\"lemma\"] in resultVerbList) or (feature[\"lemma\"] in resultVerbList):\n","      positiveFlag=True\n","    #if -ends\n","  #for -ends\n","  \n","  if not positiveFlag:\n","    return resultDict\n","  \n","  #get root\n","  rootToken = getRoot(sentence)\n","  verbSet = getVerbWithSubject(sentence)\n","  rootIndex = getTokenIndex(sentence, rootToken)\n","  \n","  #get right children of root\n","  rightChildrenList = getRightChildren(sentence, rootIndex)\n","\n","  #result\n","  for childToken in rightChildrenList:\n","    if childToken.dep_ != 'punct' and childToken.dep_!='':\n","      if childToken.lemma_ in resultNounList:\n","        childIndex = getTokenIndex(sentence, childToken)\n","        span = getPhraseSpan(sentence, childIndex)\n","        #check for existance of NER in span\n","        resultDict[\"result\"]=span\n","      #if -ends\n","    #if dep_ -ends\n","  #for -ends\n","  \n","  #ckeft children for domain and task\n","#   print(\"RootIndex_domain\",rootIndex)\n","  domainSpan=getDomain(sentence, rootIndex)\n","#   print(\"RootIndex_Task\",rootIndex)\n","  resultDict[\"domain\"] = domainSpan\n","  taskSpan = getTask(sentence, rootIndex)\n","  resultDict[\"task\"]=taskSpan\n","      \n","  return resultDict\n","# ------------------------- getResultTemplate() -ends--------------------------|"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fJi94_DnS-Vo","colab_type":"code","colab":{}},"cell_type":"code","source":["def getTerminologyTemplate(doc,sentence,featureList):\n","  resultTerminology={\"key\":\"\",\"definition\":\"\",\"reference\":\"\"}\n","  doc=nlp(sentence)\n","  p = re.compile('[A-Z]{2,}[a-z]*')\n","  chunkNodes=getNounChunks(doc)\n","  defination=\"\"\n","  for token in featureList:\n","    if p.match(str(token['token'])):\n","      term=str(token['token'])\n","      for chunks in chunkNodes:\n","        if term in str(chunks[3]):\n","#           print(term)\n","          defination=str(chunks[0])\n","          informationTerm=getPhraseSpan(sentence, getTokenIndex(sentence,token['token']))\n","          domain=str(informationTerm)[str(informationTerm).index(defination)+len(defination):]\n","          resultTerminology['key']=term\n","          resultTerminology['definition']=defination\n","          resultTerminology['reference']=domain\n","  return resultTerminology"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-ji2KoWVZsmI","colab_type":"code","colab":{}},"cell_type":"code","source":["def getAimTemplate(doc,sentence,featureList):\n","  aimDict={\"problem\":'',\"domain\":'',\"hypothesis\":''}\n","  aimVerbList=['aim','aimed','take','took','train','trained','direct','directed','purpose','purposed','purport','purported','drive','drove','get','got','calculate','calculated','target','targeted','place','placed','aspire','aspired']\n","  aimNounList=['purpose','intent','intention','design','object','bearing','heading','target']\n","  positiveFlag = False\n","  doc=nlp(sentence)\n","  for index, feature in enumerate(featureList):\n","    if (feature[\"lemma\"] in aimVerbList) or (feature[\"lemma\"] in aimVerbList):\n","      positiveFlag=True\n","      if not positiveFlag:\n","        return resultDict\n","      rootToken = getRoot(sentence)\n","      verbSet = getVerbWithSubject(sentence)\n","      rootIndex = getTokenIndex(sentence, rootToken)\n","      allChildrenList=getAllChildren(sentence,rootIndex)\n","      for x in allChildrenList:\n","        if x.dep_=='prep':\n","          aimDict['domain']=getPhraseSpan(sentence,getTokenIndex(sentence,x))\n","        if str(aimDict['domain'])=='' and x.dep_=='nsubj':\n","          aimDict['domain']=getPhraseSpan(sentence,getTokenIndex(sentence,x))\n","        if x.dep_=='xcomp':\n","          for y in getAllChildren(sentence,getTokenIndex(sentence,x)):\n","            if y.dep_=='prep':\n","              aimDict['hypothesis']=str(getPhraseSpan(sentence,getTokenIndex(sentence,y)))\n","            if y.dep_=='dobj':\n","              aimDict['problem']=x.text+\" \"+str(getPhraseSpan(sentence,getTokenIndex(sentence,y)))\n","              \n","  if not positiveFlag:\n","    return aimDict\n","  \n","  \n","  \n","  return aimDict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yCWsU3c_ZwKy","colab_type":"code","colab":{}},"cell_type":"code","source":["def getObservationTemplate(doc,sentence,featureList):\n","  observationDict={\"object\":'',\"action\":'',\"observation\":'',\"hypothesis\":''}\n","  observationVerbList={'observe','modify','consider','be','make','transfer'}\n","  doc=nlp(sentence)\n","  chunkNodeList=getNounChunks(doc)\n","  for index, feature in enumerate(featureList):\n","    if str(feature['lemma']) in observationVerbList:\n","      if str(feature['dependency'])=='ROOT':\n","        action=feature['token']\n","        observationDict['action']=action\n","        childrenList=getAllChildren(sentence,getTokenIndex(sentence,action))\n","        for x in childrenList:\n","          if 'NN' in str(x.tag_):\n","            observationDict['observation']=getPhraseSpan(sentence,getTokenIndex(sentence,x))\n","          if str(x.tag_)=='IN' or str(x.tag_)=='VBG':\n","            observationDict['hypothesis']=getPhraseSpan(sentence,getTokenIndex(sentence,x))\n","          for chunks in chunkNodeList:\n","            if 'subj' in str(chunks[2]):\n","              observationDict['object']=str(getPhraseSpan(sentence,getTokenIndex(sentence,chunks[0])))\n","              if len(observationDict['object']) < len(str(chunks[1])):\n","                observationDict['object']=str(chunks[0])\n","           \n","  return observationDict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fIuUUGqfZ0pD","colab_type":"code","colab":{}},"cell_type":"code","source":["def getSelectionTemplate(doc,sentence,featureList):\n","  selectionDict={\"sample\":'',\"stage\":'',\"sample criteria\":'',\"justification\":''}\n","  selectionVerbList={'select'}\n","  doc=nlp(sentence)\n","  chunkNodeList=getNounChunks(doc)\n","  passive=False\n","  active=False\n","  for index,feature in enumerate(featureList):\n","    if str(feature['lemma']) in selectionVerbList:\n","      if str(feature['dependency'])=='ROOT':\n","        childrenList=getAllChildren(sentence,getTokenIndex(sentence,feature['token']))\n","        for chunks in chunkNodeList:\n","          if ('subjpass' in str(chunks[2])) and (str(feature['lemma']) in str(chunks[3])):\n","            selectionDict['sample']=chunks[0]\n","          if ('nsubj'== str(chunks[2])) and (str(feature['lemma']) in str(chunks[3])):\n","            selectionDict['stage']=chunks[0]\n","            \n","        for child in childrenList:\n","          for chunks in chunkNodeList:\n","            if 'subjpass' in str(chunks[2]):\n","              passive=True\n","            if 'nsubj'==str(chunks[2]):\n","              active=True\n","            if passive and (str(child)== str(chunks[1])):\n","              childSubList=getAllChildren(sentence,getTokenIndex(sentence,child))\n","              for childSub in childSubList:\n","                for chunksSub in chunkNodeList:\n","                  if ('pobj' in str(chunksSub[2])) and (str(childSub)== str(chunksSub[3])):\n","                    selectionDict['sample criteria']=str(getPhraseSpan(sentence,getTokenIndex(sentence,childSub)))\n","            elif passive and ('pobj' in str(chunks[2])) and (str(child)== str(chunks[3])):\n","              selectionDict['stage']=str(getPhraseSpan(sentence,getTokenIndex(sentence,child)))\n","            elif passive and ('dobj' in str(chunks[2])) and (str(child)== str(chunks[3])):\n","              selectionDict['justification']=str(getPhraseSpan(sentence,getTokenIndex(sentence,child)))\n","              \n","            if active and (str(child)== str(chunks[1])):\n","              childSubList=getAllChildren(sentence,getTokenIndex(sentence,child))\n","              for childSub in childSubList:\n","                for chunksSub in chunkNodeList:\n","                  if ('dobj' in str(chunksSub[2])) and (str(childSub)== str(chunksSub[3])):\n","                    selectionDict['sample criteria']=str(getPhraseSpan(sentence,getTokenIndex(sentence,childSub)))\n","            for indexSub,featureSub in enumerate(featureList):\n","              if active and ('dobj' in str(chunks[2])) and (str(feature['lemma'])== str(chunks[3])) and featureSub['pos']=='NOUN':\n","                selectionDict['sample']=str(getPhraseSpan(sentence,getTokenIndex(sentence,child)))\n","              elif active and ('dobj' in str(chunks[2])) and (str(feature['lemma'])== str(chunks[3])) and featureSub['pos']=='VERB':\n","                selectionDict['justification']=str(getPhraseSpan(sentence,getTokenIndex(sentence,child)))\n","                 \n","  return selectionDict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nf7UrXcci52Y","colab_type":"code","colab":{}},"cell_type":"code","source":["def getContributionTask(sentence, rootIndex):\n","  task = \"\"\n","  doc = nlp(sentence)\n","  #ckeft children for domain and task\n","  leftChildrenList = getAllChildren(sentence, rootIndex)\n","  for childToken in leftChildrenList:\n","    #skip punct\n","    if childToken.dep_ != 'punct' and childToken.dep_!='':\n","\n","      #search for nsubj or nsubjpass\n","      if  childToken.dep_=='dobj' or childToken.dep_ == 'nsubjpass':\n","        taskChildIndex = getTokenIndex(sentence, childToken)\n","        if taskChildIndex!=-1:\n","          taskSpan = getPhraseSpan(sentence, taskChildIndex)\n","          task= taskSpan\n","          return task\n","      #if taskChildIndex -ends\n","      if childToken.dep_ == 'nsubj':\n","        taskChildIndex = getTokenIndex(sentence, childToken)\n","        if taskChildIndex!=-1:\n","          taskSpan = getPhraseSpan(sentence, taskChildIndex)\n","          task=taskSpan\n","          print(\"in getTask: \", task)\n","  return task"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0SLJz_Iji7IF","colab_type":"code","colab":{}},"cell_type":"code","source":["def getContributeTemplate(sentence, featureList):\n","  #initializing dictionary\n","  contributionDict = {\"research-work\":\"\", \"researchers\": \"\", \"research Time\":\"\", \"research Institute\":\"\", \"relevance\": \"\"}\n","  \n","  #initializing indecator list\n","  contributeVerbList = ['propose', 'conduct' \"approach\",'access' 'suggest','claim','nominate', 'present','contribute', 'add', 'bring','give','lead','thank', 'perform', 'execute','dedicate','devote','share','do', 'generate' 'explore','create', 'make', 'produce', 'support', 'design','work','plan','invent','formulate','study', 'identify']\n","  nounList = [\"result\", \"outcome\", \"solution\",\"effect\",'trace', 'draw','discover', 'describe','report', \"decision\", \"answer\", \"final result\",  \"fruit\", \"fruition\", \"task\", \"method\",\"research\", \"technique\", \"key\"]\n","  \n","  removeWordList = []\n","  researcherPronoun = \"\"\n","  taskThisWork = \"\"\n","  #check for indicator words\n","  positiveFlag = False\n","  for index, feature in enumerate(featureList):\n","    if feature[\"dependency\"]==\"poss\":\n","      researcherPronoun = feature[\"token\"]\n","    if feature[\"token\"].text==\"work\":\n","      taskThisWork = getPhraseSpan(sentence, index).text\n","    if (feature[\"lemma\"] in contributeVerbList) or (feature[\"lemma\"] in nounList):\n","      positiveFlag=True\n","    #if -ends\n","  #for -ends\n","  \n","  if not positiveFlag:\n","    return contributionDict\n","\n","  NERDict = getNERFeatures(doc)\n","  authorList = []\n","  for key in NERDict:\n","    if NERDict[key]==\"PERSON\":\n","      authorList.append(key)\n","      removeWordList.extend(key.lower().split())\n","    elif NERDict[key]==\"DATE\":\n","      contributionDict[\"research Time\"]=key\n","      removeWordList.extend(key.lower().split())\n","    elif NERDict[key]==\"ORG\" or NERDict[key]==\"GPE\":\n","      contributionDict[\"research Institute\"]+=\" \"+key  \n","      removeWordList.extend(key.lower().split())\n","  #for key -ends\n","  \n","  \n","  if len(authorList)!=0:\n","    contributionDict[\"researchers\"]=authorList\n","  elif not (isinstance(researcherPronoun, str)):\n","    contributionDict[\"researchers\"] = [researcherPronoun.text]\n","  else:\n","    contributionDict[\"researchers\"]=authorList\n"," \n","  \n","  #get root\n","  rootToken = getRoot(sentence)\n","  rootIndex = getTokenIndex(sentence, rootToken)\n","  verbSet = getVerbWithSubject(sentence)\n","  \n","  taskSpan = getContributionTask(sentence, rootIndex) \n","  \n","  if (isinstance(taskSpan, str))  and (\"this\".upper() in taskThisWork.upper()):\n","    contributionDict[\"research-work\"]= taskThisWork  \n","    task = taskThisWork\n","  elif (isinstance(taskSpan, str))  and (\"this\".upper() not in taskThisWork.upper()):\n","    contributionDict[\"research-work\"]= taskSpan\n","    task = taskThisWork\n","\n","  elif (str(taskSpan.root.pos_)=='PRON') and (\"this\".upper() in taskThisWork.upper()):\n","    contributionDict[\"research-work\"]= taskThisWork  \n","    task = taskThisWork\n","  else:\n","    contributionDict[\"research-work\"]= taskSpan.text\n","    task = taskSpan.text\n","  removeWordList.extend(task.lower().split())\n","\n","  \n","  relevanceSentence = removeWords(sentence, removeWordList)\n","  \n","  relevanceDoc = nlp(relevanceSentence)\n","  nounChunkList = getNounChunks(relevanceDoc)\n","  for chunk in nounChunkList:\n","    if chunk[2]==\"pobj\":\n","      contributionDict[\"relevance\"]=chunk[0]\n","    #if -ends\n","  #for -ends\n","\n","  return contributionDict\n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"JkQjnl_Z1Su1","colab_type":"code","colab":{}},"cell_type":"code","source":["def getProcessTemplate(sentence, featureList):\n","  #initializing dictionary\n","  processDict = {\"method name\":\"\", \"purpose\": \"\", \"process description\":\"\", \"assumptions\":\"\", \"tool\": \"\"}\n","  \n","  #initializing indecator list\n","  contributeVerbList = ['propose', 'conduct' \"approach\",'access', 'assess', \"perform\", 'suggest','claim', 'use', 'monitor', 'draw','discover', 'describe','utilize','apply', 'practice','nominate', 'present','contribute', 'add', 'bring','give','lead', 'perform', 'execute','devote','share','do', 'generate' 'explore','create', 'make', 'produce', 'support', 'design','work','plan','invent','formulate','study', 'identify']\n","  nounList = [\"result\", \"task\", \"monitor\", \"category\", \"input\", \"output\", \"tool\", \"frameWork\" \"project\", \"work\", \"network\",\"system\",\"treatment\",\"protocol\",\"symptoms\", \"outcome\", \"solution\",\"effect\",'trace', 'report', \"decision\", \"answer\", \"final result\",  \"fruit\", \"fruition\", \"task\", \"method\",\"research\", \"technique\", \"key\"]\n","  \n","  removeWordList = []\n","\n","  #check for indicator words\n","  positiveFlag = False\n","  for index, feature in enumerate(featureList):\n","    if (feature[\"lemma\"] in contributeVerbList) or (feature[\"lemma\"] in nounList):\n","      positiveFlag=True\n","    #if -ends\n","  #for -ends\n","  if not positiveFlag:\n","    return processDict\n"," \n","  #get root\n","  rootToken = getRoot(sentence)\n","  rootIndex = getTokenIndex(sentence, rootToken)\n","  verbSet = getAllVerbs(sentence)\n","  \n","  #get chunks\n","  doc = nlp(sentence)\n","  chunkList = getNounChunks(doc)\n","  \n","  for chunk in chunkList:\n","    #process name is subject\n","    if chunk[2]=='nsubj' or chunk[2]=='nsubjpass':\n","      processDict[\"method name\"]=getTask(sentence, rootIndex)\n","      removeWordList.extend(processDict[\"method name\"].lower().split())\n","    #if for method name -emds\n","    if chunk[3].lower()=='using' or chunk[3].lower()=='with' or chunk[3].lower()=='by' or chunk[3].lower()=='via' or chunk[3].lower()=='through':\n","      tokenIndex = getTokenIndex(sentence, chunk[3])\n","      processDict['tool']= getPhraseSpan(sentence, tokenIndex).text\n","      removeWordList.extend(processDict[\"tool\"].lower().split())\n","      \n","    #if foe tool -ends\n","    if chunk[3].lower()==\"for\" or chunk[3].lower()==\"on\" or chunk[3].lower()==\"that\" or chunk[3].lower()==\"in\":\n","      processDict[\"purpose\"]=chunk[0]\n","      removeWordList.extend(processDict[\"purpose\"].lower().split())\n","\n","    #if for purpose-ends\n","  #for -ends\n","  \n","  processSpan =\"\"\n","  for verb in verbSet:\n","    #check whether method subject is part of verb or not? WHY? Gut feeling\n","    if verb.dep_!=\"ROOT\" and verb.text!=\"using\":\n","      verbIndex = getTokenIndex(sentence, verb)\n","      childrenList = getAllChildren(sentence, verbIndex)\n","      if len(childrenList)==0:\n","        continue\n","      processSpan = getPhraseSpan(sentence, verbIndex)\n","    #if -ends\n","   \n","    if isinstance(processSpan, str):\n","      processDict[\"process description\"]=processSpan\n","    else:      \n","      processDict[\"process description\"]=processSpan.text\n","  #for -ends\n","  removeWordList.extend(processDict[\"process description\"].lower().split())\n","  \n","  relevanceSentence = removeWords(sentence, removeWordList)\n","  \n","  relevanceDoc = nlp(relevanceSentence)\n","  nounChunkList = getNounChunks(relevanceDoc)\n","  for chunk in nounChunkList:\n","    if chunk[2]==\"pobj\":\n","      processDict[\"assumptions\"]=chunk[0]\n","    #if -ends\n","  #for -ends\n","\n","\n","  return processDict\n","  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pEJtZb6kTUS3","colab_type":"code","colab":{}},"cell_type":"code","source":["def getConclusionTemplate(sentence, featureList):\n","  #initializing dictionary\n","  conclusionDict = {\"task\":\"\", \"result\": \"\", \"achievement\":\"\", \"limitations\":\"\", \"futureWork\": False}\n","  \n","  #initializing indecator list\n","  conclusionVerbList = [\"yield\", \"give\", \"render\",\"return\", \"provide\", \"generate\", \"prove\", \"predict\", \"allow\", \"permit\", \"foretell\", \"produce\", \"lead\", \"forcast\", \"report\", \"achieve\", \"accomplish\", \"reach\", \"contain\"]\n","  conclusionNounList = [\"result\", \"outcome\", \"analysis\", \"solution\",\"evidence\",\"proof\",\"insight\",\"effect\", \"decision\", \"answer\", \"final result\", \"score\", \"AP\",\"precision\", \"recall\", \"accuracy\", \"fruit\", \"fruition\", \"r\"]\n","  \n","  verbList = []\n","  #check for indicator words\n","  positiveFlag = False\n","  negationToken =\"\"\n","  for index, feature in enumerate(featureList):\n","    if (feature[\"lemma\"] in conclusionVerbList) or (feature[\"lemma\"] in conclusionNounList):\n","      positiveFlag=True\n","    if (feature[\"lemma\"] in conclusionVerbList):\n","      verbList.append(feature[\"token\"].text)\n","    if (feature[\"dependency\"]=='neg'):\n","      negationToken = feature[\"token\"]\n","    #if -ends\n","  #for -ends\n","  if not positiveFlag:\n","    return conclusionDict\n"," \n","  #get root\n","  rootToken = getRoot(sentence)\n","  rootIndex = getTokenIndex(sentence, rootToken)\n","  verbSet = getAllVerbs(sentence)\n","  \n","  #get chunks\n","  doc = nlp(sentence)\n","  chunkList = getNounChunks(doc)\n","  \n","  #achievement\n","  print(\"verbSet\", verbSet)\n","  print(\"verbList\",verbList)\n","  for verb in verbSet:\n","    if verb.text in verbList:\n","      achievementdIndex = getTokenIndex(sentence, verb)\n","      achievementdSpan = getPhraseSpan(sentence, achievementdIndex)\n","      conclusionDict[\"achievement\"]=achievementdSpan\n","      break\n","  \n","  #task\n","  subjectList = getAllSubject(sentence)\n","  taskSpan = \"\"\n","  print(\"subjectList:\", subjectList)\n","  for subject in subjectList:\n","    if subject.pos_!='PRON':\n","      taskChildIndex = getTokenIndex(sentence, subject)\n","      taskSpan = getPhraseSpan(sentence, taskChildIndex)\n","    #if -ends\n","  #for -ends\n","  if isinstance(taskSpan, str):  \n","    task = getTask(sentence, rootIndex)\n","    print(\"task: \", task)\n","  else:\n","    task = taskSpan.text\n","  #if -ends\n","  \n","  conclusionDict[\"task\"]=task\n","  \n","  #result\n","  print(\"chunkList for future\",chunkList)\n","  for chunk in chunkList:\n","    if chunk[2]=='dobj':\n","      conclusionDict[\"result\"]=chunk[0]\n","    elif chunk[2]=='pobj':\n","      conclusionDict[\"result\"]=chunk[0]\n","    #if -ends\n","    break\n","   #for -ends\n","  \n","  \n","  \n","  #future\n","  for chunk in chunkList:\n","    if \"future\" in str(chunk[0]).lower() or \"future\" in str(chunk[3]).lower():\n","      conclusionDict[\"futureWork\"]=True\n","\n","  \n","  verbSubject = getVerbSubject(sentence, rootIndex)\n","  print(\"verbSubjectList\", verbSubject)\n","  \n","  #limitations : find negation\n","  if not isinstance(negationToken, str):\n","    ancestorList = getAncestors(sentence, negationToken)\n","    print(ancestorList)\n","    ancestorIndex = getTokenIndex(sentence, ancestorList[0])\n","    conclusionDict[\"limitations\"]=getPhraseSpan(sentence,ancestorIndex)\n","    \n","    \n","    \n","  \n","  return conclusionDict\n","  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d2mZ1dl33fXi","colab_type":"code","colab":{}},"cell_type":"code","source":["def getComparisionTemplate(sentence, featureList):\n","  \n","  removeWordList = []\n","\n","  #initializing dictionary\n","  compareDict = {\"subject1\":\"\", \"subject2\": \"\", \"domain\":\"\", \"outcome\":\"\"}\n","  \n","  #initializing indecator list\n","  indicationNounList = [\"than\", \"as\", \"like\",\"more\", \"less\"]\n","    \n","  #check for indicator words\n","  positiveFlag = False\n","  indicationToken=\"\"\n","  for index, feature in enumerate(featureList):\n","    if (feature[\"lemma\"] in indicationNounList):\n","      positiveFlag=True\n","      indicationToken=feature[\"token\"]\n","    #if -ends\n","  #for -ends\n","  print(\"indicationToken\", indicationToken)\n","  \n","  #check for degree - er or est\n","  degreeWordList=re.findall(r'\\b(\\w+(er|est))\\b',sentence)\n","  \n","  \n","  if len(degreeWordList)>0:\n","    positiveFlag=True\n","    compareDict[\"outcome\"]=degreeWordList[0][0]\n","    removeWordList.extend(compareDict[\"outcome\"].lower().split())\n","\n","    \n","  if not positiveFlag:\n","    return compareDict\n"," \n","  #get root\n","  rootToken = getRoot(sentence)\n","  rootIndex = getTokenIndex(sentence, rootToken)\n","  verbSet = getAllVerbs(sentence)\n","  \n","  #get chunks\n","  doc = nlp(sentence)\n","  chunkList = getNounChunks(doc)\n","  \n","  #subject 1\n","  \n","  subjectList = getAllSubject(sentence)\n","  taskSpan = \"\"\n","  print(\"subjectList:\", subjectList)\n","  for subject in subjectList:\n","    if subject.pos_!='PRON':\n","      taskChildIndex = getTokenIndex(sentence, subject)\n","      taskSpan = getPhraseSpan(sentence, taskChildIndex)\n","    #if -ends\n","  #for -ends\n","  compareDict[\"subject1\"]=taskSpan.text\n","  removeWordList.extend(compareDict[\"subject1\"].lower().split())\n","\n","    \n","  #subject 2\n","  indicationTokenIndex= getTokenIndex(sentence, indicationToken)\n","  for chunk in chunkList:\n","    if not isinstance(indicationToken, str):\n","      if chunk[3]==indicationToken.text:\n","        compareDict[\"subject2\"]=chunk[0]\n","  \n","  removeWordList.extend(compareDict[\"subject2\"].lower().split())\n","\n","  relevanceSentence = removeWords(sentence, removeWordList)\n","  \n","  relevanceDoc = nlp(relevanceSentence)\n","  nounChunkList = getNounChunks(relevanceDoc)\n","  for chunk in nounChunkList:\n","    if chunk[2]==\"pobj\":\n","      compareDict[\"domain\"]=chunk[0]\n","    #if -ends\n","  #for -ends\n","  \n","\n","  \n","  return compareDict\n","  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"By5Xh-jrbPQg","colab_type":"code","outputId":"431c538a-b96e-471f-8be3-c51a2b227d17","executionInfo":{"status":"ok","timestamp":1544576106217,"user_tz":360,"elapsed":949,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["'''Acknowledgement'''\n","#sentence = \"Personal diet management is key to fighting the obesity epidemic.\"\n","sentence=\"A. Geiger, P. Lenz, and R. Urtasun ; “Are we ready for autonomous driving? the kitti vision benchmark suite,”; Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference;  2012; pp. 3354–3361\"\n","#sentence=\"B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3d lidar using fully convolutional network,” in Proceedings of Robotics: Science and Systems, AnnArbor, Michigan, June 2016.\"\n","\n","'''results'''\n","# sentence = \"For LID, the communication task yielded the best results detection: AUC = 0.930.\"\n","# sentence =  \"For the car class, our RPN variants achieve a 91% recall at just 50 proposals, whereas MV3D [4] reported requiring 300 proposals to achieve the same recall.\"\n","# sentence = \"As an example, our Feature Pyramid based fusion RPN achieves an 86% 3D recall on the car class with just 10 proposals per frame.\"\n","\n","'''contribution'''\n","'''working '''\n","# sentence = \"information extraction on research paper was performed by Nidhi Vaishnav and Maitri Shah in December 2018 at University of Texas at Dallas under natural language processing subject.\"\n","# sentence = \"Sir Issac Newton performed gravity experiments in 17th century at England which initiated kinematics.\"\n","# sentence = \"Presenting one of the key work in the area of 3D forensics, Kanchan Bahirat identified various possible attacks on 3D LiDAR data as well as two forgery detection methods based on consistency in the key characteristics of LiDAR data such as: sampling density and occlusion effect during 2016.\"\n","# sentence = \"Our study employed data from a cross-sectional survey and physical examinations of twins in a population-based twin registry, the Kaiser Permanente Women Twins Study Examination II, conducted in 1989 to 1990 in Oakland, California, United States.\"\n","'''accaptable'''\n","# sentence = \"This work was begun and brought to fruition when we were at Cambridge University; we dedicate this paper to the memory of Professor David G. Crighton, who first interested one of us (L.M.) in this problem nearly a decade ago.\"\n","# sentence = \"This work was supported by the European Community through Marie-Curie Fellowship HPMF-2002-01915 (to M.A.) and the U.S. Office of Naval Research through a Young Investigator Award (to L.M.).\"\n","# sentence = \"Wang et al [19] described another approach where after one wavelet decomposition, the norm and orientation of each wavelet coefficient vector are modified independently.\"\n","# sentence = \"Among the early work, Ohbuchi [23] generated a non-manifold mesh from a point cloud and performed a mesh spectral analysis for embedding watermark into spectral coefficients.\"\n","# sentence = \"Cotting designed a technique that: (a) partitions a point cloud data into patches; (b) transforms them into discrete frequency bands; (c) embeds a watermark into the lowfrequency components.\"\n","\n","'''process'''\n","sentence = \"a new interactive mobile system enables automated heart assessment based on walking and sleep cycle of user using heart rate monitor.\"\n","# sentence=\"Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space with help of Python language.\"\n","# sentence=\"In addition to using techniques in computer vision and machine learning, one unique feature of this system is the realization of real-time energy balance monitoring through metabolic network simulation.\"\n","# sentence=\"Movement trajectories of individual joints were extracted from videos of PD assessment using Convolutional Pose Machines, a pose estimation algorithm built with deep learning.\"\n","\n","''' conclusion '''\n","'''good'''\n","# sentence = \"Our analysis gave good results for templates. we achieved 75% accuracy, but it doesn't work on discourse level. We will do discourse in future\"\n","# sentence=\"Our analysis allows us to predict a critical speed for the onset of flapping as well as the frequency of flapping. But it doesn't give good critical speed for lightning bolt\"\n","# sentence=\"The proposed system provides insight into the potential of computer vision and deep learning for clinical application in PD and demonstrates promising performance for the future translation of deep learning to PD clinical practices.\"\n","'''accaptable'''\n","# sentence=\"These results provide novel evidence that lifetime socioeconomic position influences adult health and highlight the utility of studying social plus biological aspects of twinship.\"\n","# sentence=\"Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles.\"\n","\n","# ----------------------------------------------------------------Terminology-----------------------------------------------------------------------------------#\n","\n","\n","# sentence=\"When considering sparse and low resolution input such as the Front View or BEV, Bird’s Eye View point cloud projections, this method is not guaranteed to have enough information to generate region proposals, especially for small object classes.\"\n","#  sentence=\"In this paper, we aim to resolve these difficulties by proposing AVOD, an Aggregate View Object Detection architecture for autonomous driving.\"\n","# sentence=\"Nine participants with PD and LID completed a levodopa infusion protocol, where symptoms were assessed at regular intervals using the Unified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson's Disease Rating Scale (UPDRS). \"\n","# sentence=\"Features of the movement trajectories (e.g. kinematic, frequency) were used to train random forests to detect and estimate the severity of parkinsonism and LID.\"\n","##show\n","# sentence=\"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios.\"\n","# sentence=\"AR is Augment Reality\"\n","\n","\n","\n","#------------------------------------------------------------------Aim---------------------------------------------------------------------------------------#\n","## show\n","# sentence=\"In this paper, we aim to resolve these difficulties by proposing AVOD, an Aggregate View Object Detection architecture for autonomous driving.\"\n","# sentence=\"The purpose of information extraction is to make easy the question answer process by running the seekers project\"\n","# sentence=\"This study aims to put forth a new method by using georeferenced community-authored reviews for restaurants.\"\n","# sentence=\"This study aims to contribute this line of studies by proposing taste as an indicator of social status which integrates different facets of culture, economy and social networks of urban inhabitants. \"\n","# sentence=\"The objective of this paper aims to compute xyx by using first method\"\n","\n","\n","#--------------------------------------------------------------Observation--------------------------------------------------------------------------------------#\n","# sentence=\"Persons with PD and their physicians must regularly modify treatment regimens and timing for optimal relief of symptoms.\"\n","##show\n","# sentence=\"Computer vision is an attractive solution for automated assessment of PD, by making possible recent advances in computational power and deep learning algorithms.\"\n","# sentence=\"The remarkable progress made by deep neural networks on the task of 2D object detection in recent years has transferred well to the detection of objects in 3D.\"\n","# sentence=\"RPNs can be considered a weak amodal detector, providing proposals with high recall and low precision.\"\n","\n","\n","#-----------------------------------------------------------Selection-----------------------------------------------------------------------#\n","# sentence=\"We then select those features which determine one’s choice of restaurant and the rating that he/she provides for that restaurant by using best method.\"\n","##show\n","# sentence=\"Forty-five features of the three categories (i.e., foods and drinks, food adjectives, and ambience adjectives) were selected at this step using xyz method.\"\n","\n","# sentence = \"spacy is better than nltk for natural language processing task\"\n","# sentence = \"2D image detection is better than 3D image detection for Kitti dataset.\"\n","# sentence = \"3D image detection is not so good as 2D image detection for ILSVRC dataset.\"\n","# sentence=\"We accordingly compared the health status among monozygotic and dizygotic women twin pairs who lived together through childhood (until at least age 14) and subsequently were discordant or concordant on adult socioeconomic position. This comparison permitted us to ascertain the additional impact of adult experiences on adult health in a population matched on early life experiences.\"\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'accaptable'"]},"metadata":{"tags":[]},"execution_count":61}]},{"metadata":{"id":"xaUUQOUqjo3r","colab_type":"code","outputId":"8efb792e-6f89-4cc8-970a-9d9490c94252","executionInfo":{"status":"ok","timestamp":1544576107234,"user_tz":360,"elapsed":1394,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":421}},"cell_type":"code","source":["featureList = []\n","\n","tokens = getTokens(sentence)\n","print(tokens)\n","\n","doc = nlp(sentence)\n","\n","featureList = getFeatures(doc, featureList)\n","print(len(featureList))\n","\n","for i in featureList:\n","  print(i)\n","#for -ends\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['a', 'new', 'interactive', 'mobile', 'system', 'enables', 'automated', 'heart', 'assessment', 'based', 'on', 'walking', 'and', 'sleep', 'cycle', 'of', 'user', 'using', 'heart', 'rate', 'monitor', '.']\n","21\n","{'token': a, 'lemmaVal': 11901859001352538922, 'lemma': 'a', 'pos': 'DET', 'tag': 'DT', 'shape': 'x', 'isAlpha': True, 'isStop': True, 'dependency': 'det', 'headText': 'system', 'headTag': 'NOUN', 'children': []}\n","{'token': new, 'lemmaVal': 4753564829687343602, 'lemma': 'new', 'pos': 'ADJ', 'tag': 'JJ', 'shape': 'xxx', 'isAlpha': True, 'isStop': False, 'dependency': 'amod', 'headText': 'system', 'headTag': 'NOUN', 'children': []}\n","{'token': interactive, 'lemmaVal': 9613289368394214984, 'lemma': 'interactive', 'pos': 'ADJ', 'tag': 'JJ', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'amod', 'headText': 'system', 'headTag': 'NOUN', 'children': []}\n","{'token': mobile, 'lemmaVal': 13895322422246515550, 'lemma': 'mobile', 'pos': 'ADJ', 'tag': 'JJ', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'amod', 'headText': 'system', 'headTag': 'NOUN', 'children': []}\n","{'token': system, 'lemmaVal': 16001232513058686804, 'lemma': 'system', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'nsubj', 'headText': 'enables', 'headTag': 'VERB', 'children': [a, new, interactive, mobile]}\n","{'token': enables, 'lemmaVal': 1080083029942854337, 'lemma': 'enable', 'pos': 'VERB', 'tag': 'VBZ', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'ROOT', 'headText': 'enables', 'headTag': 'VERB', 'children': [system, assessment, based, .]}\n","{'token': automated, 'lemmaVal': 1159511254368363387, 'lemma': 'automate', 'pos': 'VERB', 'tag': 'VBN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'amod', 'headText': 'assessment', 'headTag': 'NOUN', 'children': []}\n","{'token': heart, 'lemmaVal': 12419898511841729938, 'lemma': 'heart', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'compound', 'headText': 'assessment', 'headTag': 'NOUN', 'children': []}\n","{'token': assessment, 'lemmaVal': 16732015447806033820, 'lemma': 'assessment', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'dobj', 'headText': 'enables', 'headTag': 'VERB', 'children': [automated, heart]}\n","{'token': based, 'lemmaVal': 4715552063986449646, 'lemma': 'base', 'pos': 'VERB', 'tag': 'VBN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'prep', 'headText': 'enables', 'headTag': 'VERB', 'children': [on]}\n","{'token': on, 'lemmaVal': 5640369432778651323, 'lemma': 'on', 'pos': 'ADP', 'tag': 'IN', 'shape': 'xx', 'isAlpha': True, 'isStop': True, 'dependency': 'prep', 'headText': 'based', 'headTag': 'VERB', 'children': [cycle]}\n","{'token': walking, 'lemmaVal': 1674876016505392235, 'lemma': 'walk', 'pos': 'VERB', 'tag': 'VBG', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'amod', 'headText': 'cycle', 'headTag': 'NOUN', 'children': [and, sleep]}\n","{'token': and, 'lemmaVal': 2283656566040971221, 'lemma': 'and', 'pos': 'CCONJ', 'tag': 'CC', 'shape': 'xxx', 'isAlpha': True, 'isStop': True, 'dependency': 'cc', 'headText': 'walking', 'headTag': 'VERB', 'children': []}\n","{'token': sleep, 'lemmaVal': 9840574412351606749, 'lemma': 'sleep', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'conj', 'headText': 'walking', 'headTag': 'VERB', 'children': []}\n","{'token': cycle, 'lemmaVal': 15920153168140029497, 'lemma': 'cycle', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'pobj', 'headText': 'on', 'headTag': 'ADP', 'children': [walking, of, using]}\n","{'token': of, 'lemmaVal': 886050111519832510, 'lemma': 'of', 'pos': 'ADP', 'tag': 'IN', 'shape': 'xx', 'isAlpha': True, 'isStop': True, 'dependency': 'prep', 'headText': 'cycle', 'headTag': 'NOUN', 'children': [user]}\n","{'token': user, 'lemmaVal': 15567826195092881809, 'lemma': 'user', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'pobj', 'headText': 'of', 'headTag': 'ADP', 'children': []}\n","{'token': using, 'lemmaVal': 6873750497785110593, 'lemma': 'use', 'pos': 'VERB', 'tag': 'VBG', 'shape': 'xxxx', 'isAlpha': True, 'isStop': True, 'dependency': 'acl', 'headText': 'cycle', 'headTag': 'NOUN', 'children': [monitor]}\n","{'token': heart, 'lemmaVal': 12419898511841729938, 'lemma': 'heart', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'compound', 'headText': 'rate', 'headTag': 'NOUN', 'children': []}\n","{'token': rate, 'lemmaVal': 17781086385965795670, 'lemma': 'rate', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'compound', 'headText': 'monitor', 'headTag': 'NOUN', 'children': [heart]}\n","{'token': monitor, 'lemmaVal': 15355159405667649896, 'lemma': 'monitor', 'pos': 'NOUN', 'tag': 'NN', 'shape': 'xxxx', 'isAlpha': True, 'isStop': False, 'dependency': 'dobj', 'headText': 'using', 'headTag': 'VERB', 'children': [rate]}\n"],"name":"stdout"}]},{"metadata":{"id":"L7x4Uq15_Shc","colab_type":"code","outputId":"b76a7ee5-b9d9-45b5-d586-566c07fe5e85","executionInfo":{"status":"ok","timestamp":1544576108552,"user_tz":360,"elapsed":2651,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"cell_type":"code","source":["# print(\"Final:{}\".format(wordnetFeatures(tokens)))\n","wordNetFeatureDict=wordnetFeatures(tokens)\n","print(wordNetFeatureDict)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['a', 'new', 'interactive', 'mobile', 'system', 'enables', 'automated', 'heart', 'assessment', 'based', 'on', 'walking', 'and', 'sleep', 'cycle', 'of', 'user', 'using', 'heart', 'rate', 'monitor', '.'] \n","\n","{'a': {'hypernym': ['metric_linear_unit', 'fat-soluble_vitamin', 'nucleotide', 'purine', 'current_unit', 'letter', 'blood_group'], 'hyponym': ['vitamin_A1', 'vitamin_A2'], 'holonym': ['nanometer', 'abampere'], 'meronym': ['picometer', 'milliampere'], 'entail': [], 'synonym': ['angstrom', 'angstrom_unit', 'A', 'vitamin_A', 'antiophthalmic_factor', 'axerophthol', 'A', 'deoxyadenosine_monophosphate', 'A', 'adenine', 'A', 'ampere', 'amp', 'A', 'A', 'a', 'A', 'type_A', 'group_A']}, 'new': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['new', 'fresh', 'new', 'novel', 'raw', 'new', 'new', 'unexampled', 'new', 'new', 'newfangled', 'new', 'New', 'Modern', 'New', 'new', 'young', 'new', 'newly', 'freshly', 'fresh', 'new']}, 'interactive': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['synergistic', 'interactive', 'interactional', 'interactive']}, 'mobile': {'hypernym': ['sculpture'], 'hyponym': [], 'holonym': ['Alabama', 'Alabama'], 'meronym': [], 'entail': [], 'synonym': ['Mobile', 'Mobile_River', 'Mobile', 'mobile', 'mobile', 'nomadic', 'peregrine', 'roving', 'wandering', 'mobile', 'mobile', 'mobile', 'fluid', 'mobile']}, 'system': {'hypernym': ['instrumentality', 'group', 'matter', 'method', 'structure', 'body_part', 'plan_of_action', 'live_body', 'orderliness'], 'hyponym': ['audio_system', 'communication_system', 'computer_system', 'containment', 'control_system', 'data_system', 'drainage_system', 'exhaust', 'explosive_detection_system', 'explosive_trace_detection', 'guidance_system', 'hookup', 'inertial_guidance_system', 'lockage', 'maze', 'mechanical_system', 'navigational_system', 'network', 'network', 'propulsion_system', 'resonator', 'scaffolding', 'security_system', 'selsyn', 'shipboard_system', 'solar_thermal_system', 'sprinkler_system', 'synchromesh', 'body', 'dragnet', 'economy', 'ecosystem', 'judiciary', 'language_system', 'machinery', 'network', 'nonlinear_system', 'organism', 'rootage', 'shebang', 'social_organization', 'solar_system', 'subsystem', 'syntax', 'water_system', 'accounting', 'anthroposophy', 'discipline', 'ethic', 'frame_of_reference', 'gambling_system', 'government', 'honor_system', 'logic', 'logic', 'merit_system', 'organon', 'point_system', 'program', 'spoils_system', 'theology', 'theosophy', 'calendar', 'classification_system', 'contrivance', 'coordinate_system', 'data_structure', 'design', 'distribution', 'genetic_map', 'kinship_system', 'lattice', 'living_arrangement', 'ontology', 'articulatory_system', 'central_nervous_system', 'digestive_system', 'endocrine_system', 'immune_system', 'integumentary_system', 'mononuclear_phagocyte_system', 'muscular_structure', 'musculoskeletal_system', 'nervous_system', 'peripheral_nervous_system', 'reproductive_system', 'respiratory_system', 'reticuloendothelial_system', 'sensory_system', 'skeletal_system', 'tract', 'urogenital_system', 'vascular_system', 'venation', 'credit_system', 'legal_system', 'pricing_system', 'promotion_system'], 'holonym': [], 'meronym': ['infrastructure', 'module', 'hierarchy'], 'entail': [], 'synonym': ['system', 'system', 'scheme', 'system', 'system', 'system_of_rules', 'arrangement', 'organization', 'organisation', 'system', 'system', 'system', 'system', 'organization', 'organisation', 'system']}, 'enables': {'hypernym': ['change'], 'hyponym': ['endow', 'equip'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['enable']}, 'automated': {'hypernym': ['change'], 'hyponym': ['semi-automatize'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['automatize', 'automatise', 'automate', 'automated', 'machine-controlled', 'machine-driven']}, 'heart': {'hypernym': ['intuition', 'internal_organ', 'courage', 'area', 'content', 'disposition', 'plane_figure', 'variety_meat', 'feeling', 'playing_card'], 'hyponym': [\"athlete's_heart\", 'biauriculate_heart', 'center_stage', 'city_center', 'financial_center', 'hub', 'inner_city', 'medical_center', 'midfield', 'midstream', 'seat', 'storm_center', 'bare_bones', 'hypostasis', 'quiddity', 'quintessence', 'stuff', 'attachment', 'protectiveness', 'regard', 'soft_spot'], 'holonym': ['circulatory_system'], 'meronym': ['cardiac_muscle', 'coronary_artery', 'heart_valve', 'valve'], 'entail': [], 'synonym': ['heart', 'bosom', 'heart', 'pump', 'ticker', 'heart', 'mettle', 'nerve', 'spunk', 'center', 'centre', 'middle', 'heart', 'eye', 'kernel', 'substance', 'core', 'center', 'centre', 'essence', 'gist', 'heart', 'heart_and_soul', 'inwardness', 'marrow', 'meat', 'nub', 'pith', 'sum', 'nitty-gritty', 'heart', 'spirit', 'heart', 'heart', 'affection', 'affectionateness', 'fondness', 'tenderness', 'heart', 'warmness', 'warmheartedness', 'philia', 'heart']}, 'assessment': {'hypernym': ['classification', 'charge', 'monetary_value', 'act'], 'hyponym': ['acid_test', 'assay', 'critical_appraisal', 'evaluation', 'reappraisal', 'underevaluation', 'tax_assessment', 'adjudication', 'disapproval', 'estimate', 'evaluation', 'justice', 'logistic_assessment', 'value_judgment'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['appraisal', 'assessment', 'assessment', 'assessment', 'judgment', 'judgement', 'assessment']}, 'based': {'hypernym': ['situate', 'drug'], 'hyponym': ['build'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['establish', 'base', 'ground', 'found', 'base', 'free-base', 'base', 'based', 'based']}, 'on': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['on', 'on', 'along', 'on', 'on', 'on']}, 'walking': {'hypernym': ['locomotion', 'travel', 'accompany', 'score', 'traverse', 'play', 'behave', 'consociate', 'pace', 'compel', 'travel'], 'hyponym': ['ambulation', 'gait', 'march', 'plodding', 'prowl', 'shamble', 'sleepwalking', 'wading', 'amble', 'ambulate', 'clump', 'flounce', 'foot', 'hike', 'limp', 'lollop', 'lumber', 'march', 'march', 'mince', 'pace', 'perambulate', 'promenade', 'prowl', 'shuffle', 'skulk', 'sleepwalk', 'slink', 'slog', 'slouch', 'sneak', 'spacewalk', 'stagger', 'stagger', 'stalk', 'step', 'stomp', 'stride', 'stroll', 'stumble', 'tap', 'tiptoe', 'tittup', 'toddle', 'toe', 'traipse', 'tramp_down', 'tread', 'wade', 'march', 'trot', 'march', 'parade', 'constitutionalize'], 'holonym': [], 'meronym': ['pace'], 'entail': ['step'], 'synonym': ['walk', 'walking', 'walk', 'walk', 'walk', 'walk', 'walk', 'walk', 'walk', 'walk', 'walk', 'walk', 'take_the_air', 'walk-to', 'walking']}, 'and': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': []}, 'sleep': {'hypernym': ['physical_condition', 'physical_condition', 'time_period', 'death', 'rest', 'accommodate'], 'hyponym': ['orthodox_sleep', 'paradoxical_sleep', 'shuteye', 'sleeping', 'beauty_sleep', 'kip', 'bundle', 'estivate', 'hibernate', 'nap', 'sleep_late'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['sleep', 'slumber', 'sleep', 'sopor', 'sleep', 'nap', 'rest', 'eternal_rest', 'sleep', 'eternal_sleep', 'quietus', 'sleep', 'kip', 'slumber', \"log_Z's\", \"catch_some_Z's\", 'sleep']}, 'cycle': {'hypernym': ['time_interval', 'series', 'repeat', 'rate', 'periodic_event', 'wheeled_vehicle', 'pass', 'pass', 'ride', 'ride', 'recur'], 'hyponym': ['merry-go-round', 'samsara', 'cardiac_cycle', 'Carnot_cycle', 'menstrual_cycle', 'pass', 'bicycle-built-for-two', 'mountain_bike', 'ordinary', 'push-bike', 'safety_bicycle', 'velocipede', 'recycle', 'cycle_on', 'backpedal', 'unicycle'], 'holonym': ['kilohertz'], 'meronym': ['phase', 'bicycle_seat', 'bicycle_wheel', 'chain', 'coaster_brake', 'handlebar', 'kickstand', 'mudguard', 'pedal', 'sprocket'], 'entail': ['kick'], 'synonym': ['cycle', 'rhythm', 'round', 'cycle', 'cycle', 'hertz', 'Hz', 'cycle_per_second', 'cycles/second', 'cps', 'cycle', 'cycle', 'oscillation', 'bicycle', 'bike', 'wheel', 'cycle', 'cycle', 'cycle', 'motorbike', 'motorcycle', 'cycle', 'bicycle', 'cycle', 'bike', 'pedal', 'wheel', 'cycle']}, 'of': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': []}, 'user': {'hypernym': ['person', 'selfish_person', 'person'], 'hyponym': ['consumer', 'end_user', 'usufructuary', 'utilizer', 'wearer', 'addict', 'head', 'tripper'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['user', 'exploiter', 'user', 'drug_user', 'substance_abuser', 'user']}, 'using': {'hypernym': ['mistreatment', 'consume', 'exploit', 'act'], 'hyponym': ['blaxploitation', 'colonialism', 'sexploitation', 'address', 'avail', 'cannibalize', 'enjoy', 'exert', 'exploit', 'exploit', 'fall_back', 'give', 'implement', 'misapply', 'overuse', 'play', 'play', 'ply', 'pull_out_all_the_stops', 'put', 'recycle', 'share', 'strain', 'take', 'waste', 'work', 'board', 'drink', 'abuse', 'pervert', 'spare', 'take', 'waste', 'trespass', 'follow'], 'holonym': [], 'meronym': [], 'entail': ['consume'], 'synonym': ['exploitation', 'victimization', 'victimisation', 'using', 'use', 'utilize', 'utilise', 'apply', 'employ', 'use', 'habituate', 'use', 'expend', 'use', 'practice', 'apply', 'use', 'use']}, 'rate': {'hypernym': ['magnitude_relation', 'charge', 'temporal_property', 'proportion', 'evaluate', 'be', 'measure'], 'hyponym': ['acceleration', 'attrition_rate', 'birthrate', 'bits_per_second', 'crime_rate', 'data_rate', 'deathrate', 'deceleration', 'dose_rate', 'erythrocyte_sedimentation_rate', 'flow', 'flux', 'frequency', 'gigahertz', 'growth_rate', 'hertz', 'inflation_rate', 'jerk', 'kilohertz', 'kilometers_per_hour', 'megahertz', 'metabolic_rate', 'miles_per_hour', 'pace', 'pulse', 'rate_of_return', 'respiratory_rate', 'revolutions_per_minute', 'sampling_rate', 'solar_constant', 'spacing', 'speed', 'tempo', 'terahertz', 'words_per_minute', 'excursion_rate', 'footage', 'freight', 'interest_rate', 'linage', 'pay_rate', 'payment_rate', 'rate_of_depreciation', 'rate_of_exchange', 'room_rate', 'tax_rate', 'beat', 'celerity', 'slowness', 'sluggishness', 'speed', 'downgrade', 'prioritize', 'reorder', 'seed', 'sequence', 'shortlist', 'subordinate', 'superordinate', 'upgrade', 'revalue'], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': ['rate', 'rate', 'charge_per_unit', 'pace', 'rate', 'rate', 'rate', 'rank', 'range', 'order', 'grade', 'place', 'rate', 'rate', 'value']}, 'monitor': {'hypernym': ['supervisor', 'defender', 'display', 'electronic_equipment', 'electronic_equipment', 'lizard', 'observe', 'observe'], 'hyponym': ['invigilator', 'computer_monitor', 'television_monitor', 'cardiac_monitor', 'electronic_fetal_monitor', 'African_monitor', 'Komodo_dragon', 'spy'], 'holonym': ['computer', 'television'], 'meronym': ['oscilloscope'], 'entail': [], 'synonym': ['proctor', 'monitor', 'admonisher', 'monitor', 'reminder', 'Monitor', 'monitor', 'monitoring_device', 'monitor', 'monitor', 'monitor', 'monitor_lizard', 'varan', 'monitor', 'supervise', 'monitor']}, '.': {'hypernym': [], 'hyponym': [], 'holonym': [], 'meronym': [], 'entail': [], 'synonym': []}}\n"],"name":"stdout"}]},{"metadata":{"id":"BP0KceVblR7d","colab_type":"code","outputId":"02049508-9a8a-435a-99bb-950f9223648a","executionInfo":{"status":"ok","timestamp":1544576108554,"user_tz":360,"elapsed":2388,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["ackTemplate = getAcknowledgementTemplate(doc, sentence)\n","print(ackTemplate)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'authors': [], 'paper': '', 'publication': '', 'publish-year': '', 'reference-Page': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"0MUgOYo60s3Z","colab_type":"code","outputId":"739284c5-796b-4c9e-b0a2-741b85ad234e","executionInfo":{"status":"ok","timestamp":1544576108558,"user_tz":360,"elapsed":1331,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["resultDict = getResultTemplate(doc, sentence, featureList)\n","print(resultDict)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'domain': '', 'task': '', 'result': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"rmyerye0nYYp","colab_type":"code","outputId":"9c6ac37b-f12f-45bb-bf58-a12ac52eef49","executionInfo":{"status":"ok","timestamp":1544576109408,"user_tz":360,"elapsed":1648,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"cell_type":"code","source":["getContributeTemplate(sentence, featureList)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'relevance': '',\n"," 'research Institute': '',\n"," 'research Time': '',\n"," 'research-work': '',\n"," 'researchers': ''}"]},"metadata":{"tags":[]},"execution_count":66}]},{"metadata":{"id":"qiLYQ4iV2Hmp","colab_type":"code","outputId":"2221e9a2-9b3a-480e-8d93-801ebeabfb47","executionInfo":{"status":"ok","timestamp":1544576109986,"user_tz":360,"elapsed":1719,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"cell_type":"code","source":["getProcessTemplate(sentence, featureList)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'assumptions': 'user monitor',\n"," 'method name': 'a new interactive mobile system',\n"," 'process description': 'walking and sleep',\n"," 'purpose': 'walking and sleep cycle',\n"," 'tool': 'using heart rate monitor'}"]},"metadata":{"tags":[]},"execution_count":67}]},{"metadata":{"id":"iTd6DKf9T8Kc","colab_type":"code","outputId":"d5201a68-12e9-45cd-ec25-04be7fbbdac6","executionInfo":{"status":"ok","timestamp":1544576109988,"user_tz":360,"elapsed":1101,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"cell_type":"code","source":["getConclusionTemplate(sentence, featureList)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'achievement': '',\n"," 'futureWork': False,\n"," 'limitations': '',\n"," 'result': '',\n"," 'task': ''}"]},"metadata":{"tags":[]},"execution_count":68}]},{"metadata":{"id":"qJmwgtfqaW1M","colab_type":"code","outputId":"0cebe51d-0234-4472-bb71-a54c4b7a014b","executionInfo":{"status":"ok","timestamp":1544576111128,"user_tz":360,"elapsed":1224,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["print(getTerminologyTemplate(doc,sentence,featureList))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'key': '', 'definition': '', 'reference': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"EaOb8F7laanU","colab_type":"code","outputId":"8cb5cd2f-7c72-40eb-c3ab-0782d1b78c67","executionInfo":{"status":"ok","timestamp":1544576113327,"user_tz":360,"elapsed":1077,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["print(getAimTemplate(doc,sentence,featureList))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'problem': '', 'domain': '', 'hypothesis': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"mLhQIB61adGo","colab_type":"code","outputId":"67b09780-4b84-4d96-a3a0-49ad44d6fdf5","executionInfo":{"status":"ok","timestamp":1544576114706,"user_tz":360,"elapsed":1650,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["print(getObservationTemplate(doc,sentence,featureList))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'object': '', 'action': '', 'observation': '', 'hypothesis': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"w8TrxpB1ah9u","colab_type":"code","outputId":"0500680d-15b4-496f-9e3a-93b40818f176","executionInfo":{"status":"ok","timestamp":1544576115382,"user_tz":360,"elapsed":1272,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["print(getSelectionTemplate(doc,sentence,featureList))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'sample': '', 'stage': '', 'sample criteria': '', 'justification': ''}\n"],"name":"stdout"}]},{"metadata":{"id":"NyCg9YKJ3n2N","colab_type":"code","outputId":"c7c97181-e8d0-4b21-a0a2-0b95d7e32e26","executionInfo":{"status":"ok","timestamp":1544576116149,"user_tz":360,"elapsed":1688,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"cell_type":"code","source":["getComparisionTemplate(sentence, featureList)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["indicationToken \n","subjectList: [system]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'domain': 'walking and sleep cycle',\n"," 'outcome': 'user',\n"," 'subject1': 'a new interactive mobile system',\n"," 'subject2': ''}"]},"metadata":{"tags":[]},"execution_count":73}]},{"metadata":{"id":"mcFp9-VLhVOe","colab_type":"code","outputId":"bd1c94ca-d323-4dc1-bc7f-54eab0a64972","executionInfo":{"status":"ok","timestamp":1544576116603,"user_tz":360,"elapsed":1477,"user":{"displayName":"nidhi vaishnav","photoUrl":"https://lh5.googleusercontent.com/-7U7uJHmGGDY/AAAAAAAAAAI/AAAAAAAAAtc/4EBXFcPtLzU/s64/photo.jpg","userId":"13611907861213894437"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"cell_type":"code","source":["displacy.render(doc, style='dep', jupyter=True, options={'distance':125})"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"953-0\" class=\"displacy\" width=\"2675\" height=\"387.0\" style=\"max-width: none; height: 387.0px; color: #000000; background: #ffffff; font-family: Arial\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">a</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"175\">new</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"175\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"300\">interactive</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"300\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"425\">mobile</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"425\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">system</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"675\">enables</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"675\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">automated</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">heart</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">assessment</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1175\">based</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1175\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1300\">on</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1300\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1425\">walking</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1425\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">and</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">CCONJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1675\">sleep</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1675\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">cycle</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1925\">of</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1925\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2050\">user</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2050\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2175\">using</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2175\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2300\">heart</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2300\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2425\">rate</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2425\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2550\">monitor.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2550\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-0\" stroke-width=\"2px\" d=\"M70,252.0 C70,2.0 550.0,2.0 550.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,254.0 L62,242.0 78,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-1\" stroke-width=\"2px\" d=\"M195,252.0 C195,64.5 545.0,64.5 545.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M195,254.0 L187,242.0 203,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-2\" stroke-width=\"2px\" d=\"M320,252.0 C320,127.0 540.0,127.0 540.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M320,254.0 L312,242.0 328,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-3\" stroke-width=\"2px\" d=\"M445,252.0 C445,189.5 535.0,189.5 535.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M445,254.0 L437,242.0 453,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-4\" stroke-width=\"2px\" d=\"M570,252.0 C570,189.5 660.0,189.5 660.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M570,254.0 L562,242.0 578,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-5\" stroke-width=\"2px\" d=\"M820,252.0 C820,127.0 1040.0,127.0 1040.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M820,254.0 L812,242.0 828,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-6\" stroke-width=\"2px\" d=\"M945,252.0 C945,189.5 1035.0,189.5 1035.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M945,254.0 L937,242.0 953,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-7\" stroke-width=\"2px\" d=\"M695,252.0 C695,64.5 1045.0,64.5 1045.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1045.0,254.0 L1053.0,242.0 1037.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-8\" stroke-width=\"2px\" d=\"M695,252.0 C695,2.0 1175.0,2.0 1175.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1175.0,254.0 L1183.0,242.0 1167.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-9\" stroke-width=\"2px\" d=\"M1195,252.0 C1195,189.5 1285.0,189.5 1285.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1285.0,254.0 L1293.0,242.0 1277.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-10\" stroke-width=\"2px\" d=\"M1445,252.0 C1445,64.5 1795.0,64.5 1795.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1445,254.0 L1437,242.0 1453,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-11\" stroke-width=\"2px\" d=\"M1445,252.0 C1445,189.5 1535.0,189.5 1535.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1535.0,254.0 L1543.0,242.0 1527.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-12\" stroke-width=\"2px\" d=\"M1445,252.0 C1445,127.0 1665.0,127.0 1665.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-12\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1665.0,254.0 L1673.0,242.0 1657.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-13\" stroke-width=\"2px\" d=\"M1320,252.0 C1320,2.0 1800.0,2.0 1800.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-13\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1800.0,254.0 L1808.0,242.0 1792.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-14\" stroke-width=\"2px\" d=\"M1820,252.0 C1820,189.5 1910.0,189.5 1910.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-14\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1910.0,254.0 L1918.0,242.0 1902.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-15\" stroke-width=\"2px\" d=\"M1945,252.0 C1945,189.5 2035.0,189.5 2035.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-15\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2035.0,254.0 L2043.0,242.0 2027.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-16\" stroke-width=\"2px\" d=\"M1820,252.0 C1820,64.5 2170.0,64.5 2170.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-16\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2170.0,254.0 L2178.0,242.0 2162.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-17\" stroke-width=\"2px\" d=\"M2320,252.0 C2320,189.5 2410.0,189.5 2410.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-17\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2320,254.0 L2312,242.0 2328,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-18\" stroke-width=\"2px\" d=\"M2445,252.0 C2445,189.5 2535.0,189.5 2535.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-18\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2445,254.0 L2437,242.0 2453,242.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-953-0-19\" stroke-width=\"2px\" d=\"M2195,252.0 C2195,64.5 2545.0,64.5 2545.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-953-0-19\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M2545.0,254.0 L2553.0,242.0 2537.0,242.0\" fill=\"currentColor\"/>\n","</g>\n","</svg>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"kKnkdT22ixV1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}